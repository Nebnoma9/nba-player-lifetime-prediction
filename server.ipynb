{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow for testing the FastAPI app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import uvicorn\n",
    "import numpy as np\n",
    "from typing import List, Union\n",
    "import nest_asyncio\n",
    "from enum import Enum\n",
    "from fastapi import FastAPI, UploadFile, File, HTTPException\n",
    "from fastapi.responses import StreamingResponse\n",
    "import joblib\n",
    "from pydantic import BaseModel, conlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create FastAPI app\n",
    "app = FastAPI(title='Deploying a nba model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represents a nba player stats data point\n",
    "class NbaPlayerStat(BaseModel):\n",
    "    gp: float\n",
    "    Min: float\n",
    "    Max: float\n",
    "    pts: float\n",
    "    fga: float\n",
    "    fg_per: float\n",
    "    three_p_made: float\n",
    "    three_pa: float\n",
    "    three_p_per: float\n",
    "    ftm: float\n",
    "    fta: float\n",
    "    ft_per: float\n",
    "    oreb: float\n",
    "    dreb: float\n",
    "    reb: float\n",
    "    ast: float\n",
    "    stl: float\n",
    "    blk: float\n",
    "    tov: float\n",
    "\n",
    "\n",
    "# Represents a batch of nba player stats\n",
    "class NbaPlayerStats(BaseModel):\n",
    "    batches: List[conlist(item_type=float, min_items=19, max_items=19)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/\")\n",
    "def home():\n",
    "    return \"Welcome to nba model API. Now head over to http://localhost:8000/docs.\"\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "def load_model():\n",
    "    # Load model from pickle file\n",
    "    global model\n",
    "    try:\n",
    "        model = joblib.load('app/nba_svc_model_v0.pkl')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the model: {e}\")\n",
    "        model = None\n",
    "    return model\n",
    "\n",
    "@app.post(\"/predict\") \n",
    "def prediction(data: Union[NbaPlayerStats, NbaPlayerStat], batch: bool):\n",
    "\n",
    "     #checking model load\n",
    "    if not model:\n",
    "        raise HTTPException(status_code=500, detail=\"Model not loaded\")\n",
    "\n",
    "    try:\n",
    "        pred = None\n",
    "        if batch:       # batch mode activated\n",
    "            # handling data\n",
    "            batches = data.batches\n",
    "            np_batches = np.array(batches)\n",
    "            #inference\n",
    "            pred = model.predict(np_batches).tolist()\n",
    "\n",
    "        else:           # no bacth\n",
    "            data_point =  np.array([\n",
    "                                        [\n",
    "                                            data.gp,\n",
    "                                            data.Min,\n",
    "                                            data.Max,\n",
    "                                            data.pts,\n",
    "                                            data.fga,\n",
    "                                            data.fg_per,\n",
    "                                            data.three_p_made,\n",
    "                                            data.three_pa,\n",
    "                                            data.three_p_per,\n",
    "                                            data.ftm,\n",
    "                                            data.fta,\n",
    "                                            data.ft_per,\n",
    "                                            data.oreb,\n",
    "                                            data.dreb,\n",
    "                                            data.reb,\n",
    "                                            data.ast,\n",
    "                                            data.stl,\n",
    "                                            data.blk,\n",
    "                                            data.tov,\n",
    "                                        ]\n",
    "                                    ]\n",
    "                                )\n",
    "            #inference\n",
    "            pred = model.predict(data_point).tolist()\n",
    "            pred = pred[0]\n",
    "\n",
    "        return {\"Prediction\": pred}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run server interactively\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [18883]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:47442 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:47442 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:33840 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:33840 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:47448 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:47448 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:47462 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:47462 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:46002 - \"POST /predict?batch=false HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:57658 - \"POST /predict?batch=false HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54968 - \"POST /predict?batch=false HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:45312 - \"POST /predict?batch=false HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [18883]\n"
     ]
    }
   ],
   "source": [
    "host = \"127.0.0.1\"\n",
    "# Spin up the server!    \n",
    "uvicorn.run(app, host=host, port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nba_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
